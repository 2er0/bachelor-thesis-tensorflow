\chapter{Facial Keypoints Detection}
\label{cha:Facial Keypoints Detection}

\section{Ausgangssituation}

Gesichtserkennung spielt im 21sten Jahrhundert eine immer größer werdende Rolle. 
So existieren Herausforderungen mit den Schlüsselpunkten im Gesicht eines Menschen, welche wieder für Gesichtserkennungen verwendet werden können. 
Im Gesicht eines Menschen wird zum Beispiel die Iris in beiden Augen als Schlüsselpunkt definiert, aber auch dich Nasenspitzen und die Mundwinkel sowie viele weitere.
Diese Schlüsselpunkte variieren sehr stark von einem Individuum zum Nächsten, aber auch jedes Individuum selbst hat eine Menge an Variationen. 
So spielt hier die Größe, die Position, die Neigung sowie die Beleuchtung eine Rolle und erzeugt eine fast unendliche Menge an Möglichkeiten. 
Mit Computer Vision konnten sehr viele Verbesserungen in diesem Bereich erzielt werden, wobei noch sehr viel Raum für Forschung und Verbesserungen bleibt. \newline

\noindent
Die Aufgabenstellung wird auf der Online-Plattform Kaggle \footnote{Kaggle: Facial Keypoints Detection \url{https://www.kaggle.com/c/facial-keypoints-detection}} gehostet, wo auch die Trainings- und Testdaten zur Verfügung gestellt werden. 
Diese Aufgabe war im Jahr 2016 eine Herausforderung, an der sich jeder beteiligen konnte, um den ersten Platz zu erreichen. 
Das Ziel für jeden Teilnehmer war es ein System zu entwickeln, welches mit den Trainingsdaten trainiert wird.
Im Anschluss sollte dieses System dann mit den Testdaten getestet werden. 
Das Ergebnis musste im Anschluss eingereicht werden, welches dann überprüft und bewertet wurde. 

\section{Vorbereitung}

Die Daten, die zur Verfügung gestellt werden, sind auf mehreren Dateien aufgeteilt. 
In diesem Fall beinhaltet die Datei mit den Trainingsdaten die meisten Daten. 
Sie beinhaltet die Bilder der Gesichter, sowie die Koordinaten der Schlüsselpunkte, gespeichert in einer CSV-Notation.
%Im gesamten Gesicht gibt es 15 Schlüsselpunkte welche hier berücksichtigt werden. 
Im gesamten Gesicht werden in diesem Beispiel 15 Schlüsselpunkte berücksichtigt. 
In der Auflistung \ref{fig:15keypoints} sind die Bezeichnungen der Spalten mit den Werten ersichtlich, wobei zu jeder Bezeichnung ein \glqq X\grqq und ein \glqq Y\grqq existiert.
Ein Beispiel dazu ist in der Auflistung \ref{fig:ausgangsdatenRoh} zu finden.
\begin{lstlisting}[caption={$15$ Schlüsselpunkte im Gesicht eines Menschen},label=fig:15keypoints,captionpos=b,language=Python]
left_eye_center, right_eye_center, 
left_eye_inner_corner, left_eye_outer_corner, right_eyee_inner_corner, right_eye_outer_corner, 
left_eyebrow_inner_end, left_eyebrow_outer_end, right_eyebrow_inner_end, right_eyebrow_outer_end, 
nose_tip, 
mouth_left_corner, mouth_right_corner, 
mouth_center_top_lip, mouth_center_bottom_lip
\end{lstlisting}
Jeder diese Schlüsselpunkte besteht aus einer X und einer Y Koordinate. 
Diese Datei besitzt zusätzlich in der $31.$ Spalte das Bild mit dem dazugehörigen Gesicht.
Das Bild ist encodiert abgelegt und besteht aus $96 * 96$ Werten. 
In diesem Sinne stehen nur Bilder in Graustufen zur Verfügung mit einer Auflösung von $96 * 96$ Pixel und einer Farbtiefe von $[0, 255]$ zur Verfügung, wie in der Abbildung \ref{fig:ausgangsdaten} zu erkennen ist. 
Für diese Aufgabe werden im Grunde auch nur die Konturen benötigt, was somit den einen Farbkanal erklärt, aber auch die Aufgabe schwieriger macht, denn mehrere Farbkanäle könnten mit Filter bearbeitet werden und so noch weiter Konturen hervorheben. 
\begin{figure}
	\centering
	\includegraphics[scale=0.75]{images/ausgangsDaten.png}
	\caption{Ein Gesicht aus dem Datenbestand mit der Verteilung der Graustufenwerten}
	\label{fig:ausgangsdaten}
\end{figure}
	\begin{lstlisting}[caption={Ein gesamter Datensatz aus den Trainingsdaten mit den X und Y Werten pro Schlüsselpunkt},label=fig:ausgangsdatenRoh,captionpos=b,language=Python]
# left\_eye\_center, right\_eye\_center
66.0335639098,39.0022736842,30.2270075188,36.4216781955
# left\_eye\_inner\_corner, left\_eye\_outer\_corner
59.582075188,39.6474225564,73.1303458647,39.9699969925
# right\_eye\_inner\_corner, right\_eye\_outer\_corner
36.3565714286,37.3894015038,23.4528721805,37.3894015038
# left\_eyebrow\_inner\_end, left\_eyebrow\_outer\_end
56.9532631579,29.0336481203,80.2271278195,32.2281383459
# right\_eyebrow\_inner\_end, right\_eyebrow\_outer\_end
40.2276090226,29.0023218045,16.3563789474,29.6474706767
# nose\_tip
44.4205714286,57.0668030075
# mouth\_left\_corner, mouth\_right\_corner
61.1953082707,79.9701654135,28.6144962406,77.3889924812
# mouth\_center\_top\_lip, mouth\_center\_bottom\_lip
43.3126015038,72.9354586466,43.1307067669,84.4857744361
# image
238 236 237 238 240 240 239 241 241 243 240 239 231 212 ...
\end{lstlisting}

\subsection{Daten vorbereiten und normalisieren}

Zu Beginn diese Daten müssen vorbereitet und normalisiert werden. 
Um die Problemgröße zu verringern, empfiehlt es sich, die Aufgabe auf mehrere Netzwerke aufzuteilen. 
Dies hat einen zusätzlichen Grund, da die Trainingsdaten nicht komplett sind und somit nicht zu allen Bildern alle 15 Schlüsselpunkte vorhanden sind. 
Sollte dies ignoriert werden, würde sich die Anzahl der zur Verfügung stehenden Datensätze von $7049$ auf $2140$ reduzieren. 
Ein weiterer Grund für die Auftrennung der Problemstellung ist, damit Aufgaben leichter verteilt werden können und die Netzwerke noch genauer angepasst werden könnten. \newline

\noindent
Unter der Zuname von Pandas \footnote{Pandas: Python Data Analysis Library \url{http://pandas.pydata.org/}} und NumPy \footnote{NumPy: Scientific Computing \url{http://www.numpy.org/}} besteht die Möglichkeit, sehr einfach und effizient auf die Datensätze zuzugreifen und diese zu verwenden. 
Wie im Codebeispiel \ref{fig:datenLesenEinschränken} ersichtlich ist, werden die Daten mit Hilfe von Pandas geladen und unter Zunahme von NumPy normalisiert und neu strukturiert. 
\begin{lstlisting}[caption={Daten einlesen und einschränken},label=fig:datenLesenEinschränken,captionpos=b,language=Python]
import pandas as pd
import numpy as np

# konstanten Definition
IMAGE_SIZE = 96

# Daten einlesen
df = pd.read_csv('~/training.csv')

# Bilder um konvertieren in eine List von Zahlen
df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))

# die Aktuell benötigten Spalten herausnehmen
df = df[['left_eye_center', 'right_eye_center', 'Image']]

# entfernen unvollständiger Datensätze
# Verringerung der Datensätze von 7049 auf 7033
df = df.dropna()

# normalisieren der Bilder in einen Wertebereich von [0, 1] 
# und überführen in eine 96 mal 96 Matrix
# Variable X beinhaltet alle Bilder des Datensatzes welche Vollständig sind
X = np.vstack(df['Image']) / 255.
X = X.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)

# explizites definieren des Datentyps für die Werte der Bilder
X = X.astype(np.float32)

# narmalisieren der Y Koordinaten in einen Wertebereich von [0, 1]
# Variable Y beinhaltet die Labels zu allen Bildern
Y = df[df.columns[:-1]].values
Y = Y / 96.0
\end{lstlisting}

\subsection{Evaluations- und Errorfunktion}

Die Ergebnisse des Netzwerkes müssen immer verglichen und validiert werden. 
In diesem Beispiel handelt es sich nicht um eine Klassifizierungsaufgabe, sondern um eine Regressionsproblemstellung. 
Deshalb kann nicht einfach eine 'Cross Entropy' Funktionen verwendet werden, um die Daten zu evaluieren und zu adaptieren. 
Aus diesem Grund muss dies manuell durchgeführt werden und selbst eine Berechnung aufgestellt werden, welche diese Werte liefert, damit diese einem Optimierer übergeben werden können. 
Der Verlust, beziehungsweise die Differenz zwischen Ergebnis des Netzwerkes und dem bekannten Ergebnis, kann durch eine Subtraktion sowie einer Quadrierung berechnet werden. 
Diese ist in der Gleichung \ref{eq:lossCalc} zu erkennen, wobei \textit{graph} eine Matrix (Tensor) an Ergebnissen ist, mit der Anzahl an Zeilen wie in der Konstante \textit{BATCH\_SIZE} definiert. 
Die Variable \textit{train\_labels\_node} beinhaltet die bekannten Ergebnisse zu Bildern mit denselben Dimensionen wie in der Matrix \textit{graph}. 
\textit{tf.subtract} führt eine Subtraktion auf jeden einzelnen Wert der beiden Matrizen aus, was auch dazu führt, dass diese dieselben Dimensionen haben müssen. 
\textit{tf.square} quadriert die berechneten Differenzen um negative Werte zu entfernen. 
Zum Abschluss werden alle Ergebnisse in der Ergebnismatrix mit \textit{tf.reduce\_sum} aufsummiert, was zu einem Skalar führt. 
Für diese konkrete Implementierung wurde als Optimierungsalgorithmus ein \textit{Adam}-Algorithmus \cite{DBLP:journals/corr/KingmaB14} verwendet. 
Um das Verlustergebnis für einen Leser lesbarer zu machen, muss der Verlustwert durch die Anzahl der Batchgröße dividiert werden, was die Differenz in einem Datensatz als Durchschnitt ergibt, ohne die Quadrierung zu berücksichtigen. 
Die gesamte Umsetzung ist im Codebeispiel \ref{fig:VerlustKonkreOpitimier} zu finden. 
\begin{equation}
	Verlust := \sum{(R - L)^2}
	\label{eq:lossCalc}
\end{equation}
\begin{lstlisting}[caption={Verlustberechnung, konkreter Opitimierungsalgorithmus, Genauigkeitsberechnung},label=fig:VerlustKonkreOpitimier,captionpos=b,language=Python]
import tensorflow as tf

# Konstanten Definition
BATCH_SIZE = 64

# Verlustberechnung
with tf.name_scope("loss"):
    # sollte sich im Laufe der Trainingsphasen an 0 annähern
    loss = tf.reduce_sum(
    			tf.square(
    				tf.subtract(graph, train_labels_node)))

# Auswahl eines konkreten Optimierungsalgorithmuses in der Kurzschreibweise
# mit einer Lernrate von 0.00001
with tf.name_scope("train"):
    train = tf.train.AdamOptimizer(learning_rate=1e-5).minimize(loss)
    
# Verlustwert durch die Anzahl der Bilder im Batch da diese 
# in der loss-Berechnung zusammen summiert werden
with tf.name_scope("accuracy"):    
    accuracy = loss / BATCH_SIZE
\end{lstlisting}
 
\section{Neuronale Ebenen vorbereiten}

Damit die Ebenen einfacher verwendet werden können, können diese als konfigurierbare Muster definiert werden. 
Dadurch wird erzielt, dass gleiche Ebenen im visualisierten Graphen dieselbe Farbe besitzen und zum anderen alle Inhalte darin zusammengefasst werden. 
Im Grunde existieren zwei verschiedene Haupttypen an Ebenen. 
Zum einen die Convolutional-Ebenen und zum anderen die Vollvernetzen-Ebenen. 
Wie im Code \ref{fig:ConvFc} ersichtlich ist, besitzen beide Hauptgruppen an Ebenen jeweils eine Datenquelle, beschrieben als \textit{x\_} und Gewichtungen und Biaseswerte. 
Die Bias-Werte werden dabei erst nach der Kernfunktion an das Ergebnis angefügt und somit erst in der Aktivierungsfunktion berücksichtigt. 
\begin{lstlisting}[caption={Definition der Convolutional- und Vollvernetzen-Ebenen},label=fig:ConvFc,captionpos=b,language=Python]
def conv_layer(x_, size_in, size_out, name="conv"):
    with tf.name_scope(name):
        weights = tf.Variable(tf.truncated_normal(
        						[3, 3, size_in, size_out], 
        						dtype=tf.float32, stddev=1e-1), 
							trainable=True, name='weights')
        conv = tf.nn.conv2d(x_, weights, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, 
								shape=[size_out], dtype=tf.float32), 
                            trainable=True, name='biases')
        bias = tf.nn.bias_add(conv, biases)
        conv = tf.nn.relu(bias, name="act")

    maxPool = tf.nn.max_pool(conv, 
                             ksize=[1, 2, 2, 1],
                             strides=[1, 2, 2, 1],
                             padding='VALID')
    return conv

def fc_layer(x_, size_out, name="fc", act=None):
    with tf.name_scope(name):
        size_in = x_.get_shape()[1].value
        weights = tf.Variable(tf.truncated_normal([size_in, size_out], 
								dtype=tf.float32, stddev=1e-2), 
							trainable=True, name='weights')
        biases = tf.Variable(tf.constant(0.0, shape=[size_out], 
								dtype=tf.float32), 
							trainable=True, name='biases')
        mul = tf.nn.xw_plus_b(x_, weights, biases)
        
        if act is not None:
            mul = act(mul)
        return mul
\end{lstlisting}

\section{Neuronale Ebenen verknüpfen}

Diese Definitionen der Ebenen aus dem Codefragment \ref{fig:ConvFc} müssen im nächsten Schritt zu einem Netzwerk zusammengesetzt werden. 
Ein neuronales Netzwerk welches, relativ leichtgewichtig ist und diese Problematik relativ brauchbar lösen kann, ist im Grunde ein sehr vereinfachtes 'VGG16' Netzwerk. \footcite{VGG16: https://arxiv.org/pdf/1409.1556.pdf}
Für diesen Fall besteht dieses aus $3$ Convolutional-Ebenen und $3$ Vollvernetzen-Ebenen, wie im Codefragment \ref{fig:buildGraph} zu sehen ist. 
Das originale VGG16 Netzwerk besitzt im Gegensatz zum aktuell verwendeten, $5$ Convolutional-Ebenen mit je $2$ integrierten Convolutional-Ebenen mit denselben Dimensionen, die Hauptebenen $3$ bis $5$ zusätzlich eine $3$ Convolutional-Ebene. 
Die letzte Ebene der Vollvernetzen-Ebenen wird dabei ohne Aktivierungsfunktion ausgeführt, um die Roh-Ergebnisse zu bekommen. 
Im aktuellen Fall werden für die $2$ Iris-Positionen im Gesicht $4$ Ergebnisse benötigt. 
An diesem Beispiel lässt sich erkennen, dass hier Variablen wiederverwendet werden, dies ist durch Python möglich und durch die Beschreibung des Graphen, welcher im Hintergrund aufgebaut und verbunden wird. 
\begin{lstlisting}[caption={Modeldefinition des Graphen},label=fig:buildGraph,captionpos=b,language=Python]
def model(data):
    net = conv_layer(data, 1, 32, "conv1")
    net = conv_layer(net, 32, 64, "conv2")
    net = conv_layer(net, 64, 128, "conv3")

    # Transformieren in eine flache Struktur
    dims = net.get_shape()[1:]
    k = dims.num_elements()
    with tf.name_scope('flatten'):
        net = tf.reshape(net, [-1, k])
    
    net = fc_layer(net, 256, "fc1", tf.nn.relu)    
    net = fc_layer(net, 256, "fc2", tf.nn.relu)
    net = fc_layer(net, 4, "fc3")
    
    return net
    
# Definition der Platzhalter für die Datenübergabe
train_data_node = tf.placeholder(tf.float32, 
						shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1))
train_labels_node = tf.placeholder(tf.float32, 
						shape=(BATCH_SIZE, 4))

# erstellen eines Graphen mit dem definierten Model
graph = model(train_data_node)
\end{lstlisting}

\section{Trainieren}

Um das erzeugte Netzwerk aus \ref{fig:buildGraph} verwenden und trainieren zu können, muss dieses zuerst initialisiert werden. 
Wie im Code \ref{fig:initRun} zu erkennen ist, wird eine globale Initialisierung verwendet, welche alle Konstanten und Variablen der aktuellen Umgebung initialisiert. 
In der For-Schleife werden fast alle Datensätze einmal durch den Graphen gesendet und entweder zum Trainieren oder Evaluieren verarbeitet. 
Der Session wird in der \textit{Run}-Methode eine Liste an Punkten des Graphen mitgegeben, welche evaluiert werden sollen. 
In der Trainingsphase ist dies der Optimierungsendpunkt und in der Evaluierungsphase die Punkte \textit{accuracy} und \textit{graph}. 
\textit{accuracy}, damit ein vergleichbarer Wert zur Verfügung steht, an welchem der Lernfortschritt erkennbar ist und der Hauptendpunkt des Graphen selbst, damit die Ergebnisse direkt in Bilder gezeichnet werden können. 
Dies ermöglicht eine visuelle Verifikation durch einen Supervisor. 
Dieses Codefragment ergibt eine sogenannte Epoche, in der alle Datensätze einmal vom Netzwerk verarbeitet werden. 
\begin{lstlisting}[caption={Initialisierung des Graphen und durchführen einer Epoche},label=fig:initRun,captionpos=b,language=Python]
# erstellen einer Session
sess = tf.Session()

# erstellen einer globalen Initialisierungsroutine
init_op = tf.global_variables_initializer()
# initialisieren aller Konstanten und Variablen
sess.run(init_op)

# durchlaufen aller Datensätze -> 1 Epoche
runing = train_data.shape[0] // BATCH_SIZE
for i in range(runing):
    offset = i * BATCH_SIZE
    
    # laden eines Datenbatches aus den Datensätzen
    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]
    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]
    
    # ersetzen der Platzhalter durch die konkreten Daten
    feed = {train_data_node: batch_data, 
            train_labels_node: batch_labels}
     
    # ausführen des Graphen zur Evaluierung
    if i % 5 == 0:
        [train_accuracy, data] = sess.run([accuracy, graph], 
        								 feed_dict=feed)
        print data[0:4]
        print batch_labels[0:4]
        
        print i, train_accuracy
    # ausführen einer Trainingsiteration
    else:
        sess.run(train, feed_dict=feed)
\end{lstlisting}

\section{Validierungsresultate}

Durch längeres Trainieren kann sich das Netzwerk entwickeln und im Lauf der Epochen möglicherweise bessere Ergebnisse liefern. 
Dies führt natürlich auch zu der Möglichkeit, dass das Netzwerk beginnt, Muster zu speichern anstatt zu lernen, auch bekannt als Overfitting. 
In der Abbildung \ref{fig:runns} sind die Ergebniszustände am Ende der ersten Epoche sowie am Ende der 800sten Epoche zu sehen. 
Im Gesamten kann nun festgestellt werden, dass das Netzwerk seine Arbeit relativ korrekt durchführt, wenn man die Größe und Dimension des Netzwerkes berücksichtigt. 
\begin{lstlisting}[caption={Ergebnisse am Ende der ersten Epoche und am Ende der $800$ Epoche},label=fig:runns,captionpos=b,language=Python]
# Ist-Ergebnis 
[[ 0.75577307  0.4491435   0.35817528  0.47219035]
 [ 0.38678363  0.22854103  0.18176009  0.24241655]
 [ 0.62869424  0.36920831  0.30089244  0.38922197]
 [ 0.57703853  0.34728855  0.27663416  0.35776597]]
# Soll-Ergebnis
[[ 0.67538255  0.36768322  0.32565034  0.41764497]
 [ 0.68114937  0.33625063  0.29470228  0.37676456]
 [ 0.6498429   0.33644043  0.34131129  0.42557183]
 [ 0.6834211   0.39188917  0.32566972  0.40061486]]
# Trainingsgenauigkeit
0.0311323
.... 800 Epochen ....
# Ist-Ergebnis
[[ 0.70133466  0.36625051  0.30835846  0.35559893]
 [ 0.68118954  0.38087887  0.30591199  0.37864769]
 [ 0.68135881  0.39241019  0.30538917  0.38810176]
 [ 0.69876605  0.37630308  0.30220476  0.42244384]]
# Soll-Ergebnis
[[ 0.72585385  0.33306885  0.38976731  0.38570897]
 [ 0.68310326  0.36581968  0.32061663  0.38434095]
 [ 0.69111852  0.4122763   0.30096296  0.41956889]
 [ 0.67226801  0.40288592  0.32735435  0.35304291]]
# Trainingsgenauigkeit
1.81956e-05
\end{lstlisting}
Zum besseren Vergleichen befindet sich in der Abbildung \ref{fig:resultPic} eine direkte Vergleichsmöglichkeit. 
Dabei wird der Sollzustand durch blaue Punkte gekennzeichnet und der Istzustand durch rote Punkte. 
Erkennbar ist hier, dass es Abweichungen zwischen den Werten gibt. 
Dies lässt sich aber durch die sehr schlanke Struktur, sowie durch die geringe Anzahl an Datensätzen erklären. 
\begin{figure}
	\centering
	\includegraphics[scale=0.5]{images/epoch-800-full.png}
	\caption{Ist- und Soll- Visualisierung des Ergebnisses}
	\label{fig:resultPic}
\end{figure}
In der Abbildung \ref{fig:lossDiag} sind die gesamten Trainingsergebnisse in Form der Verluste aufgezeichnet. 
In diesem Fall nähert sich der Verlustwert der $0$-Grenze, trotzdem scheint es gewisse Probleme zu geben. 
Eine mögliche Interpretation wäre, dass die Lernrate mit '$1e-5$' noch zu hoch definiert wurde und sich das Netzwerk so zu schnell einem lokalen Minima genähert hat, obwohl es möglicherweise ein globales Minima gegeben hätte. 
Eine andere Möglichkeit wäre, dass die Datensätze Bilder beinhalten, in welchem das Gesicht nicht vollständig zu erkennen ist oder dass diese stärker rotiert sind. 
Sollte es nicht so viel Datensätze mit diesen Eigenheiten geben, so könnte dies ein weiterer Grund sein warum das Netzwerk springt. 
Dies bedeutet, dass ein nicht stabiler Zustand vorhanden ist und möglicherweise ein Rauschen in den Daten nicht ignoriert wurde sondern darauf reagiert wird.
In diesem Diagramm lässt sich erkennen, dass die Sprünge nicht regelmäßig sind, was auf ein Mischen der Datensätze am Ende jeder Epoche zurückgeführt werden kann. 
\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/loss-diagram-all.png}
	\caption{Trainings-Verlustergebnisse in 800 Epochen}
	\label{fig:lossDiag}
\end{figure}

\section{Ergebnis}

\section{Graphenvisualisierung}

Der Graphen wurde in diesem Beispiel in ein Event-File serialisiert, dabei wurde aus Performanzgründen die Speicherung von Veränderungen in den Ebenen sowie anderer Skalar-Werten weggelassen. 
In der Abbildung \ref{fig:graphDig} ist der aktuelle Graph des Beispiels ersichtlich. 
Dieser beinhaltet die $3$ Convolutional-Ebenen, die Transformation in eine flache Struktur sowie die Vollvernetzten-Ebenen und den Optimizer sowie die Verlustberechnung. 
Im Gesamten lässt sich ein Graph erkennen, in welchem der Datenfluss am Ende beginnt und nach oben läuft. 
Am Rande der Abbildung befinden sich noch zwei zusätzliche Objekte, welche Verbindungen zu allen Objekten haben. 
Diese werden automatisch durch TensorFlow hinzugefügt und werden für die Ausführung benötigt. 
\begin{figure}
	\centering
	\includegraphics[scale=0.28]{images/graph-run.png}
	\caption{Aktueller Graphen des Beispiels}
	\label{fig:graphDig}
\end{figure}

\section{Verbesserungen}

Dieses Netzwerk stellt lediglich einen möglichen Lösungsansatz dar, mit welchem die Problemstellung verstanden werden soll. 
Im Grund existieren mehrere Möglichkeiten, um das Ergebnis des Netzwerks zu verbessern. 
Diese Möglichkeiten führen aber dazu, dass mehr Ressourcen benötigt werden. 
\begin{itemize}
	\item Verbreitern der Vollvernetzten-Ebenen, damit mehr Muster gespeichert werden können
	\item Convolutional-Ebenen doppelt ausführen 
	\item Grundanzahl an Convolutional-Ebenen erhöhen
	\item Anzahl der Datensätze durch Spiegeln verdoppeln
\end{itemize}






